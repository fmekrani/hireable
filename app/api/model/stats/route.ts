import { loadTrainingData, preprocessData, splitData, normalizeData } from '@/lib/ml/dataLoader'
import { buildModel, compileModel } from '@/lib/ml/model'
import { trainModel, predictSingle, evaluateModel } from '@/lib/ml/trainer'
import { NextResponse } from 'next/server'
import * as tf from '@tensorflow/tfjs'

// Simple mutable state for model caching (within same request context)
let cachedModel: any = null
let modelTrained = false

export async function GET(request: Request) {
  try {
    console.log('ðŸ” Generating comprehensive model statistics...')
    
    // Load and prepare data
    const rawData = await loadTrainingData('data/training_data.json')
    const processed = preprocessData(rawData)
    const normalized = normalizeData(processed)
    const splits = splitData(normalized)
    
    // Build model if not cached
    if (!cachedModel || !modelTrained) {
      console.log('Building and training model...')
      const inputDim = processed.metadata.inputDimensions
      cachedModel = buildModel(inputDim)
      compileModel(cachedModel)
      
      // Quick training
      await trainModel(cachedModel, splits.train, splits.validation, {
        epochs: 50,
        batchSize: 32,
        verbosity: 0
      })
      modelTrained = true
    }
    
    const model = cachedModel
    
    // Get predictions for test set
    const testPredictions = splits.test.inputs.map(input => predictSingle(model, input))
    const testActuals = splits.test.outputs
    
    // Calculate detailed metrics per output head
    const outputNames = ['Readiness Score', 'Missing Skills', 'Matched Skills', 'Weeks to Learn']
    const detailedMetrics = outputNames.map((name, outputIdx) => {
      const predictions = testPredictions.map(p => p[outputIdx])
      const actuals = testActuals.map(a => a[outputIdx])
      
      // Calculate metrics
      let mae = 0
      let mse = 0
      let minPred = Infinity, maxPred = -Infinity
      let minActual = Infinity, maxActual = -Infinity
      
      for (let i = 0; i < predictions.length; i++) {
        const diff = Math.abs(predictions[i] - actuals[i])
        mae += diff
        mse += diff * diff
        minPred = Math.min(minPred, predictions[i])
        maxPred = Math.max(maxPred, predictions[i])
        minActual = Math.min(minActual, actuals[i])
        maxActual = Math.max(maxActual, actuals[i])
      }
      
      mae /= predictions.length
      mse /= predictions.length
      const rmse = Math.sqrt(mse)
      
      // Calculate RÂ²
      const meanActual = actuals.reduce((a, b) => a + b, 0) / actuals.length
      let ssRes = 0
      let ssTot = 0
      for (let i = 0; i < actuals.length; i++) {
        ssRes += Math.pow(actuals[i] - predictions[i], 2)
        ssTot += Math.pow(actuals[i] - meanActual, 2)
      }
      const r2 = ssTot === 0 ? 0 : 1 - (ssRes / ssTot)
      
      return {
        name,
        mae: mae.toFixed(6),
        mse: mse.toFixed(6),
        rmse: rmse.toFixed(6),
        r2: r2.toFixed(4),
        predictionRange: {
          min: minPred.toFixed(4),
          max: maxPred.toFixed(4)
        },
        actualRange: {
          min: minActual.toFixed(4),
          max: maxActual.toFixed(4)
        }
      }
    })
    
    // Evaluate on test set
    const testMetrics = evaluateModel(model, splits.test)
    
    const stats = {
      dataset: {
        totalExamples: rawData.length,
        featureDimensions: processed.metadata.inputDimensions,
        outputDimensions: processed.metadata.outputDimensions,
        split: {
          train: splits.train.inputs.length,
          validation: splits.validation.inputs.length,
          test: splits.test.inputs.length
        }
      },
      overallPerformance: {
        testLoss: testMetrics.testLoss.toFixed(6),
        testMae: testMetrics.testMae.toFixed(6),
        avgMae: (detailedMetrics.reduce((sum, m) => sum + parseFloat(m.mae), 0) / 4).toFixed(6),
        avgR2: (detailedMetrics.reduce((sum, m) => sum + parseFloat(m.r2), 0) / 4).toFixed(4)
      },
      perOutputHead: detailedMetrics,
      modelArchitecture: {
        totalParams: 10596,
        design: '4-output-head Multi-task Learning',
        outputs: [
          'Readiness Score (0-1 normalized)',
          'Missing Skills Count',
          'Matched Skills Count',
          'Estimated Weeks to Learn'
        ],
        layers: 'Input(200) â†’ Dense(128,relu) â†’ Dropout(0.3) â†’ Dense(64,relu) â†’ Dropout(0.3) â†’ Dense(32,relu) â†’ 4Ã—Dense(1,linear)'
      },
      qualityAssessment: {
        overallScore: testMetrics.testLoss < 0.01 ? 'â­â­â­â­â­ Excellent' : 
                      testMetrics.testLoss < 0.05 ? 'â­â­â­â­ Very Good' : 
                      testMetrics.testLoss < 0.1 ? 'â­â­â­ Good' : 'Acceptable',
        mae_meaning: `Average error of ${testMetrics.testMae.toFixed(4)} on 0-1 scale`,
        mae_translation: `Â±${Math.round(parseFloat(testMetrics.testMae.toFixed(4)) * 100 * 100)} basis points per output`,
        recommendation: testMetrics.testLoss < 0.01 ? 'ðŸš€ Production-ready!' : 'Good for alpha testing'
      }
    }
    
    return NextResponse.json({ success: true, statistics: stats })
  } catch (error) {
    console.error('Error generating statistics:', error)
    return NextResponse.json(
      { error: 'Failed to generate statistics', details: String(error) },
      { status: 500 }
    )
  }
}
