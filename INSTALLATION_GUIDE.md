# ğŸš€ Hireable Development Environment Setup

## âœ… What Has Been Installed

### Python Packages (for Web Scraper)
```
âœ… requests==2.32.5        - HTTP client for fetching web pages
âœ… beautifulsoup4==4.14.3  - HTML/XML parsing
âœ… json                     - Built-in JSON support
âœ… re                       - Built-in regex support
```

### Node.js & npm
```
âœ… Node.js v25.5.0
âœ… npm v11.8.0
```

### npm Dependencies (Next.js project)
```
âœ… @tensorflow/tfjs ^4.22.0     - ML inference
âœ… next ^15.0.0                 - Web framework
âœ… react ^19.0.0                - UI library
âœ… react-dom ^19.0.0            - React DOM
âœ… lucide-react ^0.563.0        - Icons
âœ… typescript ^5.3.3            - Type checking
âœ… tailwindcss ^3.4.1           - Styling
```

---

## ğŸ› ï¸ How to Use

### Run the Python Scraper (CLI)
```bash
# Scrape Google jobs
python3 scripts/company_scraper.py --company Google --max-pages 2

# Scrape with custom config
python3 scripts/company_scraper.py --config config.json --output jobs.json

# Verbose mode for debugging
python3 scripts/company_scraper.py --company Amazon --verbose
```

### Use the API (Next.js)
```bash
# Start dev server
npm run dev

# Then in another terminal:
curl -X POST http://localhost:3000/api/scrape/jobs \
  -H "Content-Type: application/json" \
  -d '{"companyName": "Google", "maxPages": 2}'
```

### Build for production
```bash
npm run build
npm start
```

---

## âš ï¸ Important: NVM Setup for New Terminal Sessions

**NVM (Node Version Manager) was installed but requires terminal configuration.**

### Add to `~/.zshrc` (or `~/.bashrc` if using Bash):

```bash
# Add these lines to the end of your ~/.zshrc file
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
```

### Then reload your shell:
```bash
source ~/.zshrc
```

### Verify it works:
```bash
node --version   # Should output: v25.5.0
npm --version    # Should output: 11.8.0
```

---

## ğŸ“‹ Installed Files Created

### Web Scraper (Phase 5B)
- `scripts/company_scraper.py` - Core scraper with extraction logic
- `lib/scrapers/companies.ts` - TypeScript company configurations
- `lib/scrapers/companies.json` - JSON company configurations
- `app/api/scrape/jobs/route.ts` - REST API endpoint for scraping

### Setup & Config
- `requirements.txt` - Python dependencies list
- `SETUP.sh` - Automated setup script
- `package.json` - npm dependencies (already had some)

---

## ğŸ› No More Import Errors

All import warnings are now resolved:

âœ… `import requests` - âœ… Installed
âœ… `from bs4 import BeautifulSoup` - âœ… Installed
âœ… `import { NextResponse } from 'next/server'` - âœ… Available
âœ… `import { spawn } from 'child_process'` - âœ… Built-in Node.js
âœ… `import { promises as fs } from 'fs'` - âœ… Built-in Node.js
âœ… TypeScript compilation - âœ… Ready

---

## ğŸš€ Quick Start (Fresh Terminal)

```bash
# 1. Make sure NVM is loaded (or add to ~/.zshrc)
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"

# 2. Navigate to project
cd /Users/hanushgupta/Desktop/Hireable/hireable

# 3. Test scraper
python3 scripts/company_scraper.py --company Google --verbose

# 4. Start dev server
npm run dev

# 5. In another terminal, test API
curl -X POST http://localhost:3000/api/scrape/jobs \
  -H "Content-Type: application/json" \
  -d '{"companyName": "Google", "maxPages": 1}'
```

---

## ğŸ“ Troubleshooting

### Command not found: npm
**Solution:** Run `export NVM_DIR="$HOME/.nvm" && [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"`

### Command not found: python3
**Solution:** Install Python 3 via Homebrew: `brew install python3`

### SSL Warning for urllib3
**This is safe to ignore** - It's a LibreSSL warning but scraper works fine.

### npm install fails
**Solution:** Delete `package-lock.json` and `node_modules/`, then run `npm install` again

---

## âœ¨ You're All Set!

All packages and extensions are installed. No more import errors. Ready to:
- âœ… Scrape company job postings
- âœ… Extract skills, years, seniority, domain
- âœ… Serve via REST API
- âœ… Build ML prediction models
